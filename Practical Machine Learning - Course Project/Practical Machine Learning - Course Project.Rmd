---
title: "Practical Machine Learning - Course Project"
author: "Juan Agustín Morello"
date: "10/9/2020"
output: 
  html_document: 
    keep_md: yes
---

# Introduction

Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. 

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to predict the manner in which one person may do an exercise. We'll be using data collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants that were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

# Packages

This project uses the following packages:

```{r echo=TRUE, message=FALSE}
library(readr)
library(caret)
library(randomForest)
```


# Data

The data provided by the Course consists in a Training data and a Test data. Is important to keep in mind that the Test data is intended to be used to predict some outcomes not to validate the constructed model as it doesn't have the target outcomes in their observations. Later, we'll split the Training data set in both training and testing set.

### Downloading and loading datasets

```{r echo=TRUE, message=FALSE}
# Download and unzip the files

  # Make sure that the directory where the data is to be stored exist
  if(!file.exists("./data")){dir.create("./data")}


  # Create a vector with the URL address to the training dataset
  url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  # Set download path directory
  dwnld_path <- "./data/pml-training.csv"
  # Download file
  download.file(url_train, destfile=dwnld_path, method="curl")
  
  # Create a vector with the URL address to the testing dataset
  url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  # Set download path directory
  dwnld_path <- "./data/pml-testing.csv"
  # Download file
  download.file(url_test, destfile=dwnld_path, method="curl")
  
# Load data sets
  
  cols <- cols(
          user_name = col_character(),
                    raw_timestamp_part_1 = col_integer(),
          raw_timestamp_part_2 = col_integer(),
          cvtd_timestamp = col_datetime(format = "%d/%m/%Y %H:%M"),
          new_window = col_factor(c("no", "yes"), ordered=TRUE),
          num_window = col_integer(),
          
          # classe is the outcome variable
          classe = col_factor(c("A", "B", "C", "D", "E"), ordered=TRUE),
          
          #The rest of the variables are doubles
          .default = "d"
  )
  
  training <- read_csv("data/pml-training.csv", col_types=cols, na=c("", "NA",
                                                                     "#DIV/0!"))
  to_predict <- read_csv("data/pml-testing.csv", col_types=cols, na=c("", "NA",
                                                                   "#DIV/0!"))
```

### `classe`: outcome variable

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 

* **Class A**: exactly according to the specification 
* **Class B**: throwing the elbows to the front
* **Class C**: lifting the dumbbell only halfway
* **Class D**: lowering the dumbbell only halfway
* **Class E**: throwing the hips to the front


### Acknowledge

The data came from [GroupWare](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercises Dataset)

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

# Model building

## Data preprocessing

1) Split the training data set the Course provide us into two data sets: 70% into `train` and 30% into `test`. This splitting will be useful also to later compute the out-of-sample error.

```{r echo=TRUE}
set.seed(1993)
inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
train <- training[inTrain,]
test <- training[-inTrain,]
```

Next thing we have to do before creating a predictive model is filtering all the features that the data set have and find the most important ones.

2) Delete the first seven features because they do not provide important information regarded to what action is being made; the first column is the same as the index ant the rest only tell us when it was made and by who.

```{r echo=TRUE}
  train <- train[,-c(1:7)]
  test <- test[,-c(1:7)]
  to_predict <- to_predict[,-c(1:7)]
```

3) Delete all those features whose observations fall below a certain amount of NA values. We'll have as threshold a mean of 0.90 NA values. We'll erase the features first in the training data set and then delete those same features in the prediction data set.

```{r echo=TRUE}
AllNA <- sapply(train, function(x) mean(!is.na(x)) > 0.90)

train <- train[,AllNA]
test <- test[,AllNA]
to_predict <- to_predict[,AllNA]
```

4) We could try cleaning the data even further by removing the variables that are at near-zero-variance. But the `nearZeroVar` returns an empty integer variable. So we'll skip this step.

```{r echo=TRUE}
nearZeroVar(train[,-length(train)]) # All features minus outcome 'classe'
```

5) Next we'll use the `findCorrelation` function to search for highly correlated attributes with a cut off equal to 0.75.

```{r echo=TRUE}
correlation <- cor(train[, -length(train)]) # All features minus outcome 'classe'
highlyCorrelated = findCorrelation(correlation, cutoff=0.75)

train <- train[, c(highlyCorrelated, length(train))] # Correlated features plus 'classe'
test <- test[, c(highlyCorrelated, length(test))]
to_predict <- to_predict[,highlyCorrelated]
```

At the end, we end up with the following features:

```{r echo=TRUE}
names(train)
```

## Model building

Because we are interested in building a model capable of predicting a certain class, this is, our problem is about a classification outcome, in this project we'll try building a Random Forest model to predict in which class corresponds the observations and evaluate it's accuracy.

We'll instruct the `train` function to use 5-fold cross-validation on the `train` data to select optimal tuning parameters for the model (for this, we'll create a variable containing the training control options for Random Forest).

```{r echo=TRUE}
set.seed(1993)
controlRF <- trainControl(method="cv", number=5, verboseIter=FALSE)
rf_model <- train(classe ~ ., data=train, method="rf", trControl=controlRF)
rf_model$finalModel
```

Next, we'll use the `test` data to get predictions and a confusion matrix to make an analysis of the accuracy of the model.

```{r echo=TRUE}
rf_prediction <- predict(rf_model, newdata=test)
confusionMatrix(rf_prediction, test$classe)
```

This worked very well. The Random Forests model yielded highly accurate predictions on the test data set. The **accuracy** of this model on the testing dataset was is **98%** , which is a good estimate of the out-of-sample accuracy (**2%**). It’s possible that this could be improved upon by parameterizing the Random Forests method in a different fashion, but for the current purposes I’m very happy with this model.

# Predicting outcomes

Before predicting on the testing dataset provided by the Course (the 20 quiz results), it is important to train the model on the full training set, rather than using a model trained on a reduced training set (as we did with `final_train`, that it is the 70% of the original data set), in order to produce the most accurate predictions.

We'll repeat the steps of the preprocessing section using the important features found in there (the variables: `AllNA`, `highlyCorrelated`).

```{r echo=TRUE}
training <- training[,-c(1:7)]
training <- training[, AllNA]
training <- training[, c(highlyCorrelated, length(training))]
```

Next we'll build the model and predict the outcomes of the preprocessed `to_predict` data (using the previously created training control options for Random Forest).

```{r echo=TRUE}
set.seed(1993)
full_rf_model <- train(classe ~ ., data=training, method="rf",
                       trControl=controlRF)

prediction <- predict(full_rf_model, newdata=to_predict)
prediction
```
